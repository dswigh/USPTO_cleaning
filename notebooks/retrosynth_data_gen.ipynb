{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect USPTO 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "100%|██████████| 5.22M/5.22M [00:01<00:00, 4.48MiB/s]\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from tdc.generation import RetroSyn\n",
    "data = RetroSyn(name = 'USPTO-50K')\n",
    "split = data.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input                C#CCCCCC(C)(C)c1cc(OC)cc(OC)c1\n",
       "output    COc1cc(OC)cc(C(C)(C)CCCCC#C[Si](C)(C)C)c1\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split['train'].iloc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from rdkit import RDLogger \n",
    "RDLogger.DisableLog('rdApp.*')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_for_retrosynth_transformer(df, path_to_src_test, output_folder_path):\n",
    "    \"\"\"\n",
    "    df: dataframe created by ORDerly\n",
    "    src_test_list: a list of strings for the source (src, i.e. the products) in the test set. These will be removed when generating the train and val sets. \n",
    "    output_folder_path: path to where to save the txt files\n",
    "\n",
    "    \"\"\"\n",
    "    with open(path_to_src_test, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    products = [product.strip().replace(\" \", \"\") for product in lines]\n",
    "    \n",
    "    print('Canonicalising...')\n",
    "    # Double check that all the tgt_smiles are canonicalised\n",
    "    canonical_smiles_list = [Chem.MolToSmiles(Chem.MolFromSmiles(smiles)) for smiles in tqdm(products)]\n",
    "    canonical_smiles_list = list(set(canonical_smiles_list))\n",
    "    \n",
    "    # Retrosynthesis can only handle reactions where there's one product, so remove any rows containing 2 products\n",
    "    assert 'product_001' not in df.columns\n",
    "\n",
    "    # Add mask col for any rows that have product(s) contained in the test set\n",
    "    \n",
    "    # Add mask to remove any rows that have a reaction contained within canonical_smiles_list\n",
    "\n",
    "    # Add a new column 'test_set', which is True if the value in 'product_000' is in canonical_smiles_list, otherwise False\n",
    "    df['test_set'] = df['product_000'].isin(canonical_smiles_list)\n",
    "        \n",
    "        \n",
    "    # We now can create our train_val_df\n",
    "    train_val_df = df[df['test_set'] == False]\n",
    "    train_val_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    pre_drop_size = len(df)\n",
    "    post_drop_size = len(train_val_df)\n",
    "    assert pre_drop_size > post_drop_size\n",
    "    print(pre_drop_size-post_drop_size, ' of ',pre_drop_size ,' reactions dropped (',(pre_drop_size-post_drop_size)/pre_drop_size,'%)')\n",
    "    \n",
    "    \n",
    "    # Define a function to shuffle the values and concatenate them\n",
    "    def concatenate_reagents(row, cols):\n",
    "        values = [row[col] for col in cols if row[col] is not None]\n",
    "        random.shuffle(values)\n",
    "        for item in values:\n",
    "            if type(item) is not str:\n",
    "                print(row)\n",
    "        return '.'.join(values)\n",
    "    \n",
    "    # Create src col (ie the source inputs for the model, which are the reactants, solvents, and agents)\n",
    "    def create_src(df):\n",
    "        df = df.copy()\n",
    "        # Create a list of all columns beginning with \"reactant\", \"solvent\", and \"agent\"\n",
    "        reagent_cols = [col for col in df.columns if col.startswith((\"product\"))]\n",
    "        \n",
    "        df['src'] = df['product_000']\n",
    "        return df\n",
    "    \n",
    "    def create_tgt(df):\n",
    "        df = df.copy()\n",
    "        reactant_cols = [col for col in df.columns if col.startswith((\"reactant\"))]\n",
    "        \n",
    "        # Apply the function to each row and create a new 'src' column\n",
    "        df['tgt'] = df.apply(concatenate_reagents, args=(reactant_cols,), axis=1)\n",
    "        return df\n",
    "\n",
    "    print('Creating src...')\n",
    "    df = create_src(df)\n",
    "    print('Creating tgt...')\n",
    "    df = create_tgt(df)\n",
    "    \n",
    "    print('Augmenting...')\n",
    "    # augment\n",
    "    def augment_smiles_list(smiles_list):\n",
    "        new_smiles = []\n",
    "        for smiles in tqdm(smiles_list):\n",
    "            random_equivalent_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles), doRandom=True)\n",
    "            new_smiles.append(random_equivalent_smiles)\n",
    "        return smiles_list + new_smiles\n",
    "    \n",
    "    src = augment_smiles_list(list(df['src']))\n",
    "    tgt = augment_smiles_list(list(df['tgt']))\n",
    "    \n",
    "    assert len(src) == 2*len(df['src'])\n",
    "    assert len(tgt) == 2*len(df['tgt'])\n",
    "    \n",
    "    print('Tokenizing...')\n",
    "    \n",
    "    # tokenize smiles\n",
    "    def smi_tokenizer(smi):\n",
    "        \"\"\"\n",
    "        Tokenize a SMILES molecule or reaction\n",
    "        \"\"\"\n",
    "        import re\n",
    "        pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "        regex = re.compile(pattern)\n",
    "        tokens = [token for token in regex.findall(smi)]\n",
    "        assert smi == ''.join(tokens)\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def smi_tokenizer_list(smi_list):\n",
    "        new_smi_list = []\n",
    "        for smi in smi_list:\n",
    "            new_smi_list.append(smi_tokenizer(smi))\n",
    "            \n",
    "        return new_smi_list\n",
    "    \n",
    "    src_final = smi_tokenizer_list(src)\n",
    "    tgt_final = smi_tokenizer_list(tgt)\n",
    "    \n",
    "    src_tgt_df = pd.DataFrame({'src': src_final, 'tgt': tgt_final})\n",
    "    src_tgt_df = src_tgt_df.drop_duplicates()\n",
    "    # Splitting the DataFrame into training and validation sets\n",
    "    train_df, val_df = train_test_split(src_tgt_df, test_size=0.068, random_state=42)\n",
    "    \n",
    "    src_train = train_df['src'].tolist()\n",
    "    src_val = val_df['src'].tolist()\n",
    "    \n",
    "    tgt_train = train_df['tgt'].tolist()\n",
    "    tgt_val = val_df['tgt'].tolist()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # write to txt\n",
    "    with open(f'{output_folder_path}/src_train.txt', 'w') as file:\n",
    "        file.write('\\n'.join(src_train))\n",
    "    with open(f'{output_folder_path}/src_val.txt', 'w') as file:\n",
    "        file.write('\\n'.join(src_val))\n",
    "    with open(f'{output_folder_path}/tgt_train.txt', 'w') as file:\n",
    "        file.write('\\n'.join(tgt_train))\n",
    "    with open(f'{output_folder_path}/tgt_val.txt', 'w') as file:\n",
    "        file.write('\\n'.join(tgt_val))\n",
    "    \n",
    "    print('Done!')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strict dataset/ orderly benchmark / dataset D\n",
    "df = pd.read_parquet('/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_generated_datasets/strict_filtering.parquet')\n",
    "\n",
    "# convert from .txt file to list of smiles\n",
    "path_to_tgt_test = '/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_transformer_datasets/forward/dataset_D/src-test.txt'\n",
    "\n",
    "output_folder_path = '/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_transformer_datasets/forward/dataset_D/'\n",
    "\n",
    "prep_data_for_mol_transformer(df, path_to_tgt_test, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrosynth prediction dataset/ loose filtering / dataset F\n",
    "df = pd.read_parquet('/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_generated_datasets/retrosynth_data.parquet')\n",
    "\n",
    "# convert from .txt file to list of smiles\n",
    "path_to_tgt_test = '/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_transformer_datasets/retrosynth/dataset_F/src-test.txt'\n",
    "\n",
    "output_folder_path = '/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_transformer_datasets/forward/dataset_F/'\n",
    "\n",
    "prep_data_for_mol_transformer(df, path_to_tgt_test, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_generated_datasets/strict_filtering.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orderly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
