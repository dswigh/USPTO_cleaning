{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping data for molecular transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pschwllr/MolecularTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from rdkit import RDLogger \n",
    "RDLogger.DisableLog('rdApp.*')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Check that all reactions in test set only have 1 product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT mixed augmented\n",
    "path = '../../data/mol_transformer_data/MIT_mixed_augm/tgt-test.txt'\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    test_set = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'.' in test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for char in test_set:\n",
    "    if char == '.':\n",
    "        count += 1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement separate functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove reactions with a product found in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from .txt file to list of smiles\n",
    "#path = '../mol_transformer_datasets/ORDerly_mixed_augm_strict/tgt-test.txt'\n",
    "path = '/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_transformer_datasets/forward/dataset_D/tgt-test.txt'\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "products = [product.strip().replace(\" \", \"\") for product in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2726.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only keep the canonicalised smiles strings\n",
    "# Canonicalize the SMILES strings\n",
    "products = products[:10000]\n",
    "print(len(products))\n",
    "canonical_smiles_list = [Chem.MolToSmiles(Chem.MolFromSmiles(smiles)) for smiles in tqdm(products)]\n",
    "products == canonical_smiles_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1368 O=C(Nc1cc2[nH]c(-c3ccccc3)c3cn[nH]c(=O)c(c1)c23)C1CCCCN1 O=C1NN=Cc2c(-c3ccccc3)[nH]c3cc(NC(=O)C4CCCCN4)cc1c23\n"
     ]
    }
   ],
   "source": [
    "for i in range(40000):\n",
    "    if products[i] != canonical_smiles_list[i]:\n",
    "        print(i, products[i], canonical_smiles_list[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39418\n"
     ]
    }
   ],
   "source": [
    "canonical_smiles_list = list(set(canonical_smiles_list))\n",
    "print(len(canonical_smiles_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "strict_filtering = pd.read_parquet('/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_response_datasets/strict_filtering.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mask to remove any rows that have a reaction contained within canonical_smiles_list\n",
    "\n",
    "# Add a new column 'test_set', which is True if the value in 'product_000' is in canonical_smiles_list, otherwise False\n",
    "strict_filtering['test_set'] = strict_filtering['product_000'].isin(canonical_smiles_list)\n",
    "\n",
    "# if there are multiple products:\n",
    "if 'product_001' in df.columns:\n",
    "    strict_filtering['prod_0.prod_1'] = strict_filtering['product_000']+'.'+strict_filtering['product_001']\n",
    "    strict_filtering['prod_1.prod_0'] = strict_filtering['product_001']+'.'+strict_filtering['product_000']\n",
    "    strict_filtering['test_set_01'] = strict_filtering['prod_0.prod_1'].isin(canonical_smiles_list)\n",
    "    strict_filtering['test_set_10'] = strict_filtering['prod_1.prod_0'].isin(canonical_smiles_list)\n",
    "    \n",
    "    strict_filtering['test_set'] = strict_filtering['test_set'] and strict_filtering['test_set_01'] and strict_filtering['test_set_10']\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = strict_filtering[strict_filtering['test_set'] == False]\n",
    "train_val_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340285"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356906"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(strict_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16621"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "356906-340285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06833168198435205"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now create mask for validation set\n",
    "# Lets first find out how large the validation set should be\n",
    "30000/(30000+409035)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare src (ie the source inputs for the model, which are the reactants, solvents, and agents)\n",
    "def process_dataframe(df):\n",
    "    df = df.copy()\n",
    "    # Create a list of all columns beginning with \"reactant\", \"solvent\", and \"agent\"\n",
    "    reagent_cols = [col for col in df.columns if col.startswith((\"reactant\", \"solvent\", \"agent\"))]\n",
    "\n",
    "    # Define a function to shuffle the values and concatenate them\n",
    "    def concatenate_reagents(row):\n",
    "        values = [row[col] for col in reagent_cols if row[col] is not None]\n",
    "        random.shuffle(values)\n",
    "        for item in values:\n",
    "            if type(item) is not str:\n",
    "                print(row)\n",
    "        return '.'.join(values)\n",
    "\n",
    "    # Apply the function to each row and create a new 'src' column\n",
    "    df['src'] = df.apply(concatenate_reagents, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_dataframe(train_val_df)\n",
    "# Splitting the DataFrame into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.068, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of strings for src and tgt\n",
    "src_list = df['src'].tolist()\n",
    "tgt_list = df['product_000'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nc1cccc2cnc(Cl)cc12',\n",
       " 'CCCCC[C@H](O)C=CC1CCC(=O)C1CC=CCCCC(=O)O',\n",
       " 'CCOC(=O)C=C1Cc2ccccc2N(C)c2ccc(SCC)cc21',\n",
       " 'CCSc1ccc2c(c1)C(CC(=O)O)=Cc1ccccc1N2C',\n",
       " 'Clc1ccc2nc3n(c2c1)CCC3',\n",
       " 'O=C(O)c1ccc2c(c1)nc1n2CCC1',\n",
       " 'CC(C)(C)N=NC(C)(C#N)C1CC1',\n",
       " 'CC(C)(C)N=NC(C)(C#N)CC(=O)O',\n",
       " 'O=C(O)c1ccc(NC(=O)C2CC2)cc1[N+](=O)[O-]',\n",
       " 'O=C(C=Cc1ccccc1)NN1CC(=O)NC1=O']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[OH-].CC(=O)O.[Na+].[Fe].O=[N+]([O-])c1cccc2cnc(Cl)cc12.O',\n",
       " 'Cl.CCO.CCCCC[C@H](O)C=CC1C=CC(=O)C1CC=CCCCC(=O)O',\n",
       " 'CCOC(=O)CC1(O)Cc2ccccc2N(C)c2ccc(SCC)cc21.CCO.Cl',\n",
       " 'Cl.[K+].CCO.CCOC(=O)C=C1Cc2ccccc2N(C)c2ccc(SCC)cc21.[OH-]',\n",
       " 'Nc1ccc2nc3n(c2c1)CCC3.O=N[O-].Cl.O.[Na+]',\n",
       " '[Na+].CC(=O)O.CCO.[OH-].CCOC(=O)c1ccc2c(c1)nc1n2CCC1',\n",
       " 'BrBr.CC(C)(C)NNC(C)(C#N)C1CC1.O.ClCCl',\n",
       " 'O.CO.[OH-].[Na+].CCOC(=O)CC(C)(C#N)N=NC(C)(C)C',\n",
       " 'C1CCOC1.O=C(Cl)C1CC1.Nc1ccc(C(=O)O)c([N+](=O)[O-])c1.[Na+].O.[OH-]',\n",
       " 'NN1CC(=O)NC1=O.Cl.O=C(Cl)C=Cc1ccccc1.c1ccncc1']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment\n",
    "def augment_smiles_list(smiles_list, n=10):\n",
    "    new_smiles = []\n",
    "    for smiles in tqdm(smiles_list):\n",
    "        random_equivalent_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles), doRandom=True)\n",
    "    return smiles_list + new_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1740.71it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1673.53it/s]\n"
     ]
    }
   ],
   "source": [
    "src_list_aug = augment_smiles_list(src_list[:100])\n",
    "tgt_list_aug = augment_smiles_list(tgt_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize smiles\n",
    "def smi_tokenizer(smi):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES molecule or reaction\n",
    "    \"\"\"\n",
    "    import re\n",
    "    pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "    assert smi == ''.join(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def smi_tokenizer_list(smi_list):\n",
    "    new_smi_list = []\n",
    "    for smi in smi_list:\n",
    "        new_smi_list.append(smi_tokenizer(smi))\n",
    "        \n",
    "    return new_smi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_final = smi_tokenizer_list(src_list_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to txt\n",
    "with open('output_file_using_join.txt', 'w') as file:\n",
    "    file.write('\\n'.join(src_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement overall function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_for_mol_transformer(df, path_to_tgt_test, output_folder_path):\n",
    "    \"\"\"\n",
    "    df: dataframe created by ORDerly\n",
    "    tgt_test_list: a list of strings for the targents (tgt, i.e. the products) in the test set. These will be removed when generating the train and val sets.\n",
    "    output_folder_path: path to where to save the txt files\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(path_to_tgt_test, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    products = [product.strip().replace(\" \", \"\") for product in lines]\n",
    "    \n",
    "    print('Canonicalising...')\n",
    "    # Double check that all the tgt_smiles are canonicalised\n",
    "    canonical_smiles_list = [Chem.MolToSmiles(Chem.MolFromSmiles(smiles)) for smiles in tqdm(products)]\n",
    "    canonical_smiles_list = list(set(canonical_smiles_list))\n",
    "\n",
    "    # Add mask col for any rows that have product(s) contained in the test set\n",
    "    \n",
    "    # Add mask to remove any rows that have a reaction contained within canonical_smiles_list\n",
    "\n",
    "    # Add a new column 'test_set', which is True if the value in 'product_000' is in canonical_smiles_list, otherwise False\n",
    "    df['test_set'] = df['product_000'].isin(canonical_smiles_list)\n",
    "\n",
    "    # if there are multiple products:\n",
    "    if 'product_001' in df.columns:\n",
    "        df['prod_0.prod_1'] = df['product_000']+'.'+df['product_001']\n",
    "        df['prod_1.prod_0'] = df['product_001']+'.'+df['product_000']\n",
    "        df['test_set_01'] = df['prod_0.prod_1'].isin(canonical_smiles_list)\n",
    "        df['test_set_10'] = df['prod_1.prod_0'].isin(canonical_smiles_list)\n",
    "        \n",
    "        df['test_set'] = df['test_set'] + df['test_set_01'] + df['test_set_10']\n",
    "        \n",
    "        \n",
    "    # We now can create our train_val_df\n",
    "    train_val_df = df[df['test_set'] == False]\n",
    "    train_val_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    pre_drop_size = len(df)\n",
    "    post_drop_size = len(train_val_df)\n",
    "    assert pre_drop_size > post_drop_size\n",
    "    \n",
    "    \n",
    "    # Define a function to shuffle the values and concatenate them\n",
    "    def concatenate_reagents(row, cols):\n",
    "        values = [row[col] for col in cols if row[col] is not None]\n",
    "        random.shuffle(values)\n",
    "        for item in values:\n",
    "            if type(item) is not str:\n",
    "                print(row)\n",
    "        return '.'.join(values)\n",
    "    \n",
    "    # Create src col (ie the source inputs for the model, which are the reactants, solvents, and agents)\n",
    "    def create_src(df):\n",
    "        df = df.copy()\n",
    "        # Create a list of all columns beginning with \"reactant\", \"solvent\", and \"agent\"\n",
    "        reagent_cols = [col for col in df.columns if col.startswith((\"reactant\", \"solvent\", \"agent\"))]\n",
    "\n",
    "        # Apply the function to each row and create a new 'src' column\n",
    "        df['src'] = df.apply(concatenate_reagents, args=(reagent_cols,), axis=1)\n",
    "        return df\n",
    "    \n",
    "    def create_tgt(df):\n",
    "        df = df.copy()\n",
    "        prod_cols = [col for col in df.columns if col.startswith((\"product\"))]\n",
    "        \n",
    "        if len(prod_cols) == 1:\n",
    "            df['tgt'] = df['product_000']\n",
    "        else:\n",
    "            df['tgt'] = df.apply(concatenate_reagents, args=(prod_cols,), axis=1)\n",
    "        return df\n",
    "    print('Creating src...')\n",
    "    df = create_src(df)\n",
    "    print('Creating tgt...')\n",
    "    df = create_tgt(df)\n",
    "    \n",
    "    print('Augmenting...')\n",
    "    # augment\n",
    "    def augment_smiles_list(smiles_list):\n",
    "        new_smiles = []\n",
    "        for smiles in tqdm(smiles_list):\n",
    "            random_equivalent_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles), doRandom=True)\n",
    "            new_smiles.append(random_equivalent_smiles)\n",
    "        return smiles_list + new_smiles\n",
    "    \n",
    "    src = augment_smiles_list(list(df['src']))\n",
    "    tgt = augment_smiles_list(list(df['tgt']))\n",
    "    \n",
    "    assert len(src) == 2*len(df['src'])\n",
    "    assert len(tgt) == 2*len(df['tgt'])\n",
    "    \n",
    "    print('Tokenizing...')\n",
    "    \n",
    "    # tokenize smiles\n",
    "    def smi_tokenizer(smi):\n",
    "        \"\"\"\n",
    "        Tokenize a SMILES molecule or reaction\n",
    "        \"\"\"\n",
    "        import re\n",
    "        pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "        regex = re.compile(pattern)\n",
    "        tokens = [token for token in regex.findall(smi)]\n",
    "        assert smi == ''.join(tokens)\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def smi_tokenizer_list(smi_list):\n",
    "        new_smi_list = []\n",
    "        for smi in smi_list:\n",
    "            new_smi_list.append(smi_tokenizer(smi))\n",
    "            \n",
    "        return new_smi_list\n",
    "    \n",
    "    src_final = smi_tokenizer_list(src)\n",
    "    tgt_final = smi_tokenizer_list(tgt)\n",
    "    \n",
    "    src_tgt_df = pd.DataFrame({'src': src_final, 'tgt': tgt_final})\n",
    "    src_tgt_df = src_tgt_df.drop_duplicates()\n",
    "    # Splitting the DataFrame into training and validation sets\n",
    "    train_df, val_df = train_test_split(src_tgt_df, test_size=0.068, random_state=42)\n",
    "    \n",
    "    src_train = train_df['src'].tolist()\n",
    "    src_val = val_df['src'].tolist()\n",
    "    \n",
    "    tgt_train = train_df['tgt'].tolist()\n",
    "    tgt_val = val_df['tgt'].tolist()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # write to txt\n",
    "    with open(f'{output_folder_path}/src-train.txt', 'w') as file:\n",
    "        file.write('\\n'.join(src_train))\n",
    "    with open(f'{output_folder_path}/src-val.txt', 'w') as file:\n",
    "        file.write('\\n'.join(src_val))\n",
    "    with open(f'{output_folder_path}/tgt-train.txt', 'w') as file:\n",
    "        file.write('\\n'.join(tgt_train))\n",
    "    with open(f'{output_folder_path}/tgt-val.txt', 'w') as file:\n",
    "        file.write('\\n'.join(tgt_val))\n",
    "    \n",
    "    print('Done!')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canonicalising...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:14<00:00, 2747.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating src...\n",
      "Creating tgt...\n",
      "Augmenting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 356906/356906 [03:50<00:00, 1545.09it/s]\n",
      "100%|██████████| 356906/356906 [02:16<00:00, 2614.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Strict dataset/ orderly benchmark / dataset D\n",
    "df = pd.read_parquet('/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_generated_datasets/strict_filtering.parquet')\n",
    "\n",
    "# convert from .txt file to list of smiles\n",
    "path_to_tgt_test = '/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_transformer_datasets/forward/dataset_D/tgt-test.txt'\n",
    "\n",
    "output_folder_path = '/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_transformer_datasets/forward/dataset_D/'\n",
    "\n",
    "prep_data_for_mol_transformer(df, path_to_tgt_test, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canonicalising...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:15<00:00, 2588.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating src...\n",
      "Creating tgt...\n",
      "Augmenting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 919231/919231 [10:22<00:00, 1476.72it/s]\n",
      "100%|██████████| 919231/919231 [05:52<00:00, 2608.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# forward prediction dataset/ mid filtering / dataset E\n",
    "df = pd.read_parquet('/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_generated_datasets/forward_pred_data.parquet')\n",
    "\n",
    "# convert from .txt file to list of smiles\n",
    "path_to_tgt_test = '/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_transformer_datasets/forward/dataset_E/tgt-test.txt'\n",
    "\n",
    "output_folder_path = '/Users/dsw46/Projects_local/orderly_reviewer_response/orderly_transformer_datasets/forward/dataset_E/'\n",
    "\n",
    "prep_data_for_mol_transformer(df, path_to_tgt_test, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orderly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
