{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from condition_prediction.run import ConditionPrediction\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "wandb_entity=\"ceb-sre\"\n",
    "wandb_project=\"orderly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update models with top_n accuracy scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through all relevant runs on wandb to get run_ids, datasets and random seeds\n",
    "# For each rerun the conditionprediction with skip_training=True and resume=True\n",
    "DATASETS = [\"with_trust_with_map\",\"with_trust_no_map\", \"no_trust_no_map\", \"no_trust_with_map\"]\n",
    "BASE_PATH = pathlib.Path(\"/project/studios/orderly-preprocessing/ORDerly/\")\n",
    "DATASETS_PATH = BASE_PATH / \"data/orderly/datasets/\"\n",
    "MODEL_PATH = pathlib.Path(\"ORDerly/models\")\n",
    "configs =[]\n",
    "for random_seed in [12345, 54321,98765]:\n",
    "    for dataset in DATASETS:\n",
    "        filters = {\n",
    "            \"state\": \"finished\",\n",
    "            \"config.output_folder_path\": {\"$in\":\n",
    "                [\n",
    "                    f\"models/{dataset}\", \n",
    "                    str(MODEL_PATH / dataset),\n",
    "                    f\"/Users/Kobi/Documents/Research/phd_code/ORDerly/models/{dataset}\"\n",
    "                ],\n",
    "            },\n",
    "            \"config.random_seed\": random_seed,\n",
    "            # \"config.train_fraction\": 1.0,\n",
    "            \"config.dataset_version\": \"v4\",\n",
    "            \"config.train_mode\": 0, # Teacher forcing\n",
    "        }\n",
    "        runs = api.runs(\n",
    "            f\"{wandb_entity}/{wandb_project}\",\n",
    "            filters=filters\n",
    "        )\n",
    "        if not len(runs) == 5: # For 5 training fractions\n",
    "            raise ValueError(f\"Not 5 runs for {dataset} (found {len(runs)})\")\n",
    "        \n",
    "        for run in runs:\n",
    "            config = dict(run.config)\n",
    "            train_data_path = pathlib.Path(f\"{DATASETS_PATH}/orderly_{dataset}_train.parquet\")\n",
    "            test_data_path = pathlib.Path(f\"{DATASETS_PATH}/orderly_{dataset}_test.parquet\")\n",
    "            fp_directory = train_data_path.parent / \"fingerprints\"\n",
    "            train_fp_path = fp_directory / (train_data_path.stem + \".npy\")\n",
    "            test_fp_path = fp_directory / (test_data_path.stem + \".npy\")\n",
    "            output_folder_path = MODEL_PATH / dataset\n",
    "            output_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "            tags = dataset.split(\"_\")\n",
    "            tags = [f\"{tags[0]}_{tags[1]}\", f\"{tags[2]}_{tags[3]}\"]\n",
    "            config.update({\n",
    "                \"train_data_path\": train_data_path,\n",
    "                \"test_data_path\": test_data_path,\n",
    "                \"train_fp_path\": train_fp_path,\n",
    "                \"test_fp_path\": test_fp_path,\n",
    "                \"output_folder_path\": output_folder_path,\n",
    "                \"skip_training\": True,\n",
    "                \"resume\": True,\n",
    "                \"resume_from_best\": True,\n",
    "                \"generate_fingerprints\": False,\n",
    "                \"wandb_run_id\": run.id,\n",
    "                \"wandb_tags\": tags,\n",
    "            })\n",
    "            configs.append(config)\n",
    "            del config[\"n_val\"]\n",
    "            del config[\"n_test\"]\n",
    "            del config[\"n_train\"]\n",
    "            del config[\"dataset_version\"]\n",
    "            instance = ConditionPrediction(**config)\n",
    "            instance.run_model_arguments()\n",
    "            wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test_best'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m run \u001b[39m=\u001b[39m runs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[39m# Get model solvent, agent and overall accuracy\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m test_best \u001b[39m=\u001b[39m run\u001b[39m.\u001b[39;49msummary[\u001b[39m\"\u001b[39;49m\u001b[39mtest_best\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     20\u001b[0m solvent_accuracy \u001b[39m=\u001b[39m test_best[\u001b[39m\"\u001b[39m\u001b[39msolvent_accuracy\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     21\u001b[0m agent_accuracy \u001b[39m=\u001b[39m test_best[\u001b[39m\"\u001b[39m\u001b[39mthree_agents_accuracy\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/old/summary.py:109\u001b[0m, in \u001b[0;36mSummarySubDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    106\u001b[0m     k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget(k)  \u001b[39m# load the value into _dict if it should be there\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dict[k]\n\u001b[1;32m    111\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test_best'"
     ]
    }
   ],
   "source": [
    "DATASETS = [\"with_trust_with_map\",\"with_trust_no_map\", \"no_trust_no_map\", \"no_trust_with_map\"]\n",
    "lines = [\"Solvents\", \"Agents\", \"Everything\"]\n",
    "for dataset in DATASETS:\n",
    "    filters = {\n",
    "        \"state\": \"finished\",\n",
    "        \"config.output_folder_path\": f\"models/{dataset}\",\n",
    "        \"config.random_seed\": 12345,\n",
    "        \"config.train_fraction\": 1.0,\n",
    "        \"config.train_mode\": 0, # Teacher forcing\n",
    "    }\n",
    "    runs = api.runs(\n",
    "        f\"{wandb_entity}/{wandb_project}\",\n",
    "        filters=filters\n",
    "    )\n",
    "    assert len(runs) == 1\n",
    "    run = runs[0]\n",
    "\n",
    "    # Get model solvent, agent and overall accuracy\n",
    "    test_best = run.summary[\"test_best\"]\n",
    "    solvent_accuracy = test_best[\"solvent_accuracy\"]\n",
    "    agent_accuracy = test_best[\"three_agents_accuracy\"]\n",
    "    overall_accuracy = test_best[\"overall_accuracy\"]\n",
    "\n",
    "    # Get frequency informed solvent, agent and overall accuracy\n",
    "    fi_solvent_accuracy = run.summary[\"frequency_informed_solvent_accuracy\"]\n",
    "    fi_agent_accuracy = run.summary[\"frequency_informed_agent_accuracy\"]\n",
    "    fi_overall_accuracy = run.summary[\"frequency_informed_overall_accuracy\"]\n",
    "\n",
    "    # Improvement\n",
    "    solvent_improvement = (solvent_accuracy-fi_solvent_accuracy)/fi_solvent_accuracy\n",
    "    solvent_improvement_color = \"lessgreen\" if solvent_improvement>0 else \"red\"\n",
    "    agent_improvement = (agent_accuracy-fi_agent_accuracy)/fi_agent_accuracy\n",
    "    agent_improvement_color = \"lessgreen\" if agent_improvement>0 else \"red\"\n",
    "    overall_improvement = (overall_accuracy-fi_overall_accuracy)/fi_overall_accuracy\n",
    "    overall_improvement_color = \"lessgreen\" if overall_improvement>0 else \"red\"\n",
    "\n",
    "    # Create table lines\n",
    "    lines[0] += f\" & {fi_solvent_accuracy*100:.0f} // {solvent_accuracy*100:.0f} // \\\\textcolor{{{solvent_improvement_color}}}{{{solvent_improvement*100:.0f}\\%}} \"\n",
    "    lines[1] += f\" & {fi_agent_accuracy*100:.0f} // {agent_accuracy*100:.0f} // \\\\textcolor{{{agent_improvement_color}}}{{{agent_improvement*100:.0f}\\%}} \"\n",
    "    lines[2] += f\" & {fi_overall_accuracy*100:.0f} // {overall_accuracy*100:.0f} // \\\\textcolor{{{overall_improvement_color}}}{{{overall_improvement*100:.0f}\\%}} \"\n",
    "print(\"\\\\\\\\ \\n\".join(lines) + \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\"with_trust_with_map\",\"with_trust_no_map\", \"no_trust_no_map\", \"no_trust_with_map\"]\n",
    "LABELS = {\n",
    "    \"with_trust_with_map\": r\"Labelling, rare $\\rightarrow$ other\",\n",
    "    \"with_trust_no_map\": r\"Labelling, rare $\\rightarrow$ delete rxn\",\n",
    "    \"no_trust_no_map\": r\"Reaction string, rare $\\rightarrow$ other\",\n",
    "    \"no_trust_with_map\": r\"Reaction string, rare $\\rightarrow$ delete rxn\",\n",
    "}\n",
    "TRAIN_FRACS =  [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "fig, ax = plt.subplots(1)\n",
    "markers = [\"o\", \"d\", \"s\", \"^\"]\n",
    "for i, dataset in enumerate(DATASETS):\n",
    "    overall_accuracies = []\n",
    "    for train_fraction in TRAIN_FRACS:\n",
    "        filters = {\n",
    "            \"state\": \"finished\",\n",
    "            \"config.output_folder_path\": f\"models/{dataset}\",\n",
    "            \"config.random_seed\": 12345,\n",
    "            \"config.train_fraction\": train_fraction,\n",
    "            \"config.train_mode\": 0, # Teacher forcing\n",
    "        }\n",
    "        runs = api.runs(\n",
    "            f\"{wandb_entity}/{wandb_project}\",\n",
    "            filters=filters\n",
    "        )\n",
    "        assert len(runs) == 1\n",
    "        run = runs[0]\n",
    "\n",
    "        # Get overall accuracy\n",
    "        acc = run.summary[\"test_best\"][\"overall_accuracy\"]\n",
    "        overall_accuracies.append(acc)\n",
    "    \n",
    "    # Add line to plot\n",
    "    label = LABELS[dataset]\n",
    "    ax.plot(\n",
    "        TRAIN_FRACS, \n",
    "        overall_accuracies, \n",
    "        label=label, \n",
    "        linewidth=3.5, \n",
    "        marker=markers[i], \n",
    "        markersize=10,\n",
    "    )\n",
    "\n",
    "# Formatting\n",
    "axis_fontsize = 16\n",
    "heading_fontsize = 18\n",
    "ax.legend(loc=\"upper left\", fontsize=axis_fontsize)\n",
    "ax.set_xlabel(\"Training set fraction\", fontsize=heading_fontsize)\n",
    "ax.set_ylabel(\"Overall Accuracy\",  fontsize=heading_fontsize)\n",
    "ax.set_xticks(TRAIN_FRACS)\n",
    "ax.set_xticklabels(TRAIN_FRACS, fontsize=axis_fontsize)\n",
    "ylabels = np.arange(0.1, 0.35, 0.05)\n",
    "ax.set_yticks(ylabels)\n",
    "ax.set_yticklabels([f\"{ylabel:0.2f}\" for ylabel in ylabels], fontsize=axis_fontsize)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"scaling_behavior.png\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condition_prediction",
   "language": "python",
   "name": "conditionprediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
